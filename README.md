# openbench-nebius-guide

Endâ€‘toâ€‘End Guide â–¸ Evaluating NebiusÂ AIÂ Studio models with OpenBench

This walkthrough shows youâ€”from a clean shell promptâ€”to a complete, logged mmlu evaluation on a Nebiusâ€‘hosted Metaâ€‘Llamaâ€‘3.1â€‘70Bâ€‘Instruct model. Every step has been tested on macOSÂ 13Â (AppleÂ Silicon); Linux users can substitute identical commands. Windowsâ€¯âŒ¨ï¸ users can run under WSL or PowerShell.

1Â Â Prerequisites

Requirement

Why you need it

NebiusÂ AIÂ Studio account & APIÂ key

Authenticates your model calls. Grab a key at studio.nebius.comÂ â–¶Â SettingsÂ â–¶Â APIÂ keys.

curl or wget

To bootstrap uv quickly.

Rust toolâ€‘chain

Optional. uv distributes preâ€‘built wheels; if one isnâ€™t available for your platform, compiling will require RustÂ â‰¥Â 1.77.

Git

To clone the OpenBench repo (recommended).

ğŸ—’ï¸ Terminology: uv is a blisterâ€‘fast Python package/virtualâ€‘env manager fromÂ Astral. Weâ€™ll use it instead of pip+venv.

2Â Â Install uv (â‰ˆÂ 15Â s)

# macOS/Linux â€“ oneâ€‘liner
curl -LsSf https://astral.sh/uv/install.sh | sh

# Add ~/.local/bin to PATH *for this shell* (add to ~/.zshrc later)
export PATH="$HOME/.local/bin:$PATH"

# Verify
uv --version   # âœ uv 0.8.x

3Â Â Install a managed Python runtime

# Pick a modern CPython (OpenBench supports 3.10+)
uv python install 3.12      # downloads ~40Â MB once

4Â Â Clone or download OpenBench

git clone https://github.com/groq/openbench.git
cd openbench

(If you canâ€™t gitâ€‘clone, download the ZIP from GitHub and unzip to ~/openbench).

5Â Â Create & activate a virtual environment

uv venv --python 3.12       # creates .venv and pins Python
source .venv/bin/activate   # prompt changes â†’ (openbench)

6Â Â Install OpenBench & deps (â‰ˆÂ 20Â s)

uv pip install -e .         # â€œeditableâ€ install; omit -e if you prefer

7Â Â Configure environment variables

# 7.1 Nebius creds
export NEBIUS_API_KEY="<pasteâ€‘yourâ€‘keyâ€‘here>"

# 7.2 Tell OpenBench / Inspect to use a plainâ€‘vanilla OpenAIâ€‘compatible endpoint
export OPENAI_BASE_URL="https://api.studio.nebius.com/v1/"

# 7.3 (Optional) raise concurrency limits for faster evals
export INSPECT_MAX_CONNECTIONS=40

Headsâ€‘up (zsh): Put backâ€‘slashes at the end of each line when you split long commands; otherwise zsh interprets # as historyâ€expansion rather than a comment and throws â€œbadÂ patternâ€ errors.

8Â Â Dryâ€‘run sanity check (5Â samples)

bench eval mmlu \
  --model openai/meta-llama/Meta-Llama-3.1-70B-Instruct \
  --limit 5

You should see a quick accuracy score (~0â€“1 depending on RNG) and a log file path like logs/2025â€‘07â€‘31T22â€‘39â€‘24+02â€‘00_mmlu_â€¦.

9Â Â Full MMLU sweep (â‰ˆâ€¯35Â min on 70â€¯B model @Â 400â€¯KÂ TPM)

bench eval mmlu \
  --model openai/meta-llama/Meta-Llama-3.1-70B-Instruct \
  --temperature 0.6 \
  --max-connections 40 \
  --timeout 30000 \
  --logfile logs/full_mmlu_run.jsonl

Explanation of key flags

Flag

Purpose

--temperature 0.6

Matches Nebius docsâ€™ default playground setting.

--max-connections 40

Opens up toÂ 40 parallel requests (respect Nebius rate limits).

--timeout 30000

30Â s per request before retrying/aborting.

--logfile

Streams JSONL traces to a file you can parse later.

The run emits a live progress bar; expect ~60Â k requests for full MMLU.

10Â Â Inspect results

# View aggregate metrics inâ€‘terminal
bench view logs/full_mmlu_run.jsonl

# Or open the rich TUI viewer (arrowâ€‘keys navigate)
bench view

The output summarises accuracy, stem_accuracy (caseâ€‘insensitive), token counts, and perâ€‘subject breakdowns.

11Â Â Advanced knobs & tips

Goal

Flag or envâ€‘var

Notes

Evaluate another Nebius model

--model openai/deepseek/DeepSeek-R1

Use names from the Nebius dashboard.

Fewâ€‘shot / custom system prompt

--system "You are an expert â€¦"

Any OpenAI param accepted by Inspect.

Reâ€‘run failed samples only

--retry-on-error

Great for transient 5xx errors.

Limit working memory

--max-tokens 2048

Useful on very longâ€‘context models.

Nonâ€‘interactive CI run

--log-buffer 0 --logfile results.jsonl --log-level WARN

Ideal for GitHub Actions.

12Â Â Troubleshooting

Symptom

Fix

ValueError: Model API nebius â€¦ not recognised

Ensure you used the openai/ prefix (OpenBench maps Nebius to the OpenAI API schema).

zsh: bad pattern: #

Move inlineÂ # comments to their own line or escape them with \#.

429Â rateâ€‘limit errors

Dial down --max-connections, or request higher limits in Nebius Studio.

Slow run

Lower temperature, raise max-connections, ensure local bandwidth isnâ€™t saturated.

Cleanup

deactivate        # leave venv
uv python uninstall 3.12      # if you no longer need it
uv self uninstall             # removes uv binaries

13Â Â Next steps

Explore additional OpenBench benchmarks (bench list).

Compare models sideâ€‘byâ€‘side by commaâ€‘separating names: --model openai/meta-llama/70b,openai/deepseek/R1.

PushÂ results to your own dashboard with inspect_aiâ€™s reporting hooks.

Happy benchmarkingÂ ğŸš€

ğŸ¬ Quick LinkedIn demoÂ (â‰¤â€¯60â€¯s)

Need something snappy for a screenâ€‘capture? Run a tiny HumanEval sweep that still shows the flashy progress UI and spits out an accuracy score.

# Inside the (openbench) venv
bench eval humaneval \
  --model openai/meta-llama/Meta-Llama-3.1-70B-Instruct \
  --limit 3 \            # just 3 coding tasks â†’ finishes in â‰ˆ15â€¯s
  --temperature 0.4 \
  --max-connections 10 \
  --timeout 30000 \
  --logfile logs/demo_humaneval.jsonl

Watch the live tokens & progress barâ€”perfect bâ€‘roll.

When it completes, pop up the summary TUI:

bench view logs/demo_humaneval.jsonl

The colourful table makes a great closing frame.



Need to abort? Press q in the TUI or Ctrl+C in the shell.

Alternative quick demos

Benchmark

Command snippet

Typical runtime*

simpleqa (factual)

bench eval simpleqa --model â€¦ --limit 10

10â€‘20â€¯s

Miniâ€‘mmlu

bench eval mmlu --model â€¦ --limit 25

30â€‘40â€¯s

*Measured on Nebius Metaâ€‘Llamaâ€‘3.1â€‘70Bâ€‘Instruct @Â 400â€¯kâ€¯TPM with 40 concurrent connections.

Happy filmingÂ ğŸ“¹

